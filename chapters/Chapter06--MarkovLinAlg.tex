\chapter{Markov Chains and Linear Algebra}

In this chapter, we shall revisit the topic of Markov chains discussed previously, this time 
focussing on methods to use linear algebra for the analysis of Markov chains. As we shall see, 
this is a surprisingly powerful way of reasoning about many things; one can infer a large number 
of things from the transition matrix of a Markov chain. \\
Furthermore, using linear algebra for these purposes, it is possibe to design highly performant 
algorithms even when the input data is high-dimensional. 

\section{Linear Algebra}
	
	\subsection{Review of Fundamentals}
	Recall from Part 1 that we think of probabilty distributions over an enumerable sample 
	space $\Omega$ as vectors; that is, ordered collections of real numbers expressing 
	functions $\Omega \rightarrow \mathbb{R}$. Recall also that a \emph{vector space} is 
	defined as follows:
	\begin{definition}[Vector Space]
		A \emph{vector space} $(V,+,\bullet)$ over a field $F$ (i.e.\ a set with defined 
		addition, subtraction, multiplication and division operators) is a triple of an 
		underlying set $V$ with the operations $+ \in V \times V \rightarrow V$ (addition) 
		and $\bullet$ (scalar multiplication) staisfying the axioms of 
		\begin{itemize}
			\item associativity and commutativity of addition,
			\item the existence of an identity element $\mathbf{0} \in V$ with respect 
			to addition and additive inverses in $V$, denoted $- \mathbf{v}$, for any 
			\emph{vector} $\mathbf{v} \in V$,
			\item compatibility of scalar multiplication with filed multiplication;
			$a \bullet (b \bullet \mathbf{v}) = (a\cdot_F b) \bullet \mathbf{v}$ and 
			\item the existence of an identity $1 \in F$ with respect to scalar 
			multiplication, as well as distributivity of scala multipication with 
			respect to both vector and field addition.
		\end{itemize}
	\end{definition}
	We now define the concept of an \emph{inner product space}.
	\begin{definition}[Inner Product Space]
		An \emph{inner product space} $(V, +, \bullet, \langle \cdot, \cdot\rangle)$ is a 
		vector space with respect to field $F \subseteq \mathbb{C}$, equipped with a map 
		$\langle\cdot, \cdot\rangle \in V \times V \rightarrow F$ called the inner product 
		satisfying
		\begin{itemize}
			\item conjugate symmetry, i.e.\ 
			$$	
				\forall \mathbf{v}, \mathbf{w} \in V. 
				\langle \mathbf{v},\mathbf{w} \rangle = 
				\langle \mathbf{w}, \mathbf{v} \rangle^*
			$$
			\item linearity in the first argument, i.e.\ 
			$$
			\forall a \in F, \mathbf{v}, \mathbf{w}, \mathbf{u} \in V. 
			\langle a\bullet\mathbf{v} + \mathbf{w}, \mathbf{u}\rangle =
			a\cdot\langle \mathbf{v}, \mathbf{u}\rangle + 
			\langle \mathbf{w}, \mathbf{u}\rangle.
			$$
			\item positive-definiteness, i.e.\ 
			$$
				\forall \mathbf{v} \in V \setminus \{\mathbf{0}\}.
				\langle\mathbf{v}, \mathbf{v} \rangle > 0,
			$$
			which is meaningful since conjugate symmetry implies $\langle\mathbf{v}, 
			\mathbf{v} \rangle = \langle\mathbf{v}, \mathbf{v} \rangle^*$, 
			i.e.\ $\langle\mathbf{v}, \mathbf{v} \rangle \in \mathbb{R}$.
		\end{itemize}
	\end{definition}
	\begin{comment}
		We will further denote, for the typical vector space where $V = \Omega \rightarrow
		\mathbb{R}$ with respect to real numbers,
		$\langle\cdot, \cdot\rangle_\pi = \lambda\mathbf{v},\mathbf{w}\ .\ \sum_{\omega\in
		\Omega} \mathbf{v}(\omega)\cdot\mathbf{w}(\omega)\cdot\pi(\omega)$, for some fixed
		$\pi \in \Omega \rightarrow \mathbb{R}^{+}$. With this definition it is easy 
		to see that this construct satisfies the axioms for an inner product. We also note
		that, if $\pi$ is a probability distribution, then $\langle f, g \rangle_\pi = 
		\mathbb{E}_\pi(f\cdot g)$, and that with this definition the sxioms for inner 
		product spaces are indeed satisfied.
	\end{comment}

	We also introduce the notion of orthogonality: vectors $\mathbf{v}, \mathbf{w} \in V$ are
	orthogonal with respect to an inner product space $(V,+,\bullet,\langle\cdot,\cdot\rangle)$
	(with respect to the scalar field $F$) if $\langle\mathbf{v},\mathbf{w}\rangle=0$ (where,
	in full generality, $0$ denotes the additive identity in $F$). In this case, we write 
	$\mathbf{v} \bot \mathbf{w}$.

	\emph{Notation.}~We will denote the inner product space $l_{2}(\Omega, \pi) \coloneqq 
	(\Omega \rightarrow \mathbb{R}, +, \cdot, \langle \cdot , \cdot \rangle_\pi)$, where 
	addition and scalar multiplication are defined in the natural way. If we write $f \in 
	l_2(\Omega, \pi)$, what we mean is $f \in \Omega \rightarrow \mathbb{R}$ and that we have 
	the intention to use the inner product $\langle \cdot,\cdot \rangle_\pi$ on it. 

	Furthermore, we define $L^p_\pi \coloneqq (\Omega\rightarrow\mathbb{R},\|\cdot\|_{p,\pi})$, 
	where $p$ is a positive real number, to be the set of vectors in the inner product space 
	$l_2(\Omega,\pi)$ equipped with the so-called \emph{$L^p_\pi$-norm}, defined by
	$$
		\|\cdot\|_{p,\pi}=\lambda f\in\Omega\rightarrow\mathbb{R}\ .\ 
		\left(\sum_{\omega\in\Omega} | f(\omega)|^p \pi(\omega) \right)^{\sfrac{1}{p}}
	$$
	where, in particular, we have that $\|f\|_{2,\pi} = \sqrt{\langle f, f\rangle_\pi}$, and 
	we call some family of vectors $f_1, f_2 \hdots$ \emph{orthonormal} w.r.t.\ an inner 
	product space if its elements are pairwise orthogonal, and each element $f_i$ satisifies 
	$\|f_i\|_{2,\pi} = 1$.
	
	Note that any such the $L_\pi^p$-norm in some inner product space gives rise to a metric 
	on the underlying vector space, called the $L^p_\pi$-metric, defined by 
	$$
		\|\cdot - \cdot\|_{p,\pi} \coloneqq \lambda f, g \in \Omega \rightarrow \mathbb{R}
		\ .\ \|(f + (-g))\|_{p, \pi}
	$$
	where of course the additive inverse of any $g \in \Omega \rightarrow \mathbb{R}$ is given 
	by $-g \coloneqq \lambda \omega \in \Omega\ .\ -g(\omega)$.

	Of course, we may consider more elaborate operations than addition on the vectors of the
	inner product spce $l_2(\Omega, \pi)$. A \emph{linear} such unary operator M, i.e.\ one 
	that satisfies $M (a f + b g) = a M(f) + b M(g)$, can be represented using a 
	\emph{matrix}\footnote{The two notions, are, in fact, equivalent.}
	$\mathbf{M} \in \Omega^2 \rightarrow \mathbb{C}$. In this context, we define the matrix 
	multiplication of $\mathbf{M}$ with $f$ to be the result of the operation $M(f)$ or $(f)M$ 
	(we take such $M$ to be both pr{\ae}fix and postfix operators):
	\begin{align*}
		\forall f \in\Omega\rightarrow\mathbb{R}\ .\  M(f)\equiv\lambda \omega\in\Omega\ .\ 
		\sum_{\nu\in\Omega} \mathbf{M}(\omega, \nu) \cdot f(\nu) \\
		\forall f \in\Omega\rightarrow\mathbb{R}\ .\ (f)M\equiv\lambda \omega\in\Omega\ .\ 
		\sum_{\nu\in\Omega} \mathbf{M}(\nu, \omega) \cdot f(\nu)
	\end{align*}

	This leads us to formally define the concept of \emph{self-adjointness}:
	\begin{definition}
		For an enumerable sample space $\Omega$, a matrix $\mathbf{M} \in \Omega^2 
		\rightarrow \mathbb{C}$, and the operation it represents on an inner product space 
		$l_2(\Omega, \pi)$, are \emph{self-adjoint} with respect to this inner product 
		space if for any two vectors $f, g\in \Omega\rightarrow\mathbb{R}$, 
		$$		
			\langle M(f), g \rangle_\pi = \langle f, M(g) \rangle
		$$
	\end{definition}
	\begin{lemma}
		A matrix $\mathbf{M}\in\Omega^2\rightarrow\mathbb{C}$ is self-adjoint with respect
		to $l_2(\Omega, \pi)$ if, and only if, it is Hermitian, i.e.\ it is its
		own conjugate transpose.
	\end{lemma}
	\begin{proof}
		By the conjugate symmetry axiom, we have that
		\begin{align*}
			&& \langle f, M(g) \rangle_\pi &= \langle M(f) , g \rangle_\pi  \\
			\iff&& 
			\langle M(g), f\rangle_\pi^* &= \langle M(f),g\rangle_\pi 
			\\
			\iff&& 
			\left(
			\sum_{\omega,\nu\in\Omega}\mathbf{M}(\omega,\nu)g(\nu)f(\omega)\pi(\omega)
			\right)^*
			&=\sum_{\omega,\nu\in\Omega}
			\mathbf{M}(\omega,\nu)f(\nu)g(\omega)\pi(\omega) \\
			\iff&&  
			\sum_{\omega,\nu\in\Omega}
			\mathbf{M}(\omega,\nu)^*g(\nu)f(\omega)\pi(\omega) &=
			\sum_{\omega,\nu\in\Omega}
			\mathbf{M}(\omega,\nu)f(\nu)g(\omega)\pi(\omega)
			\\
			\iff&& 
			\forall \nu,\omega \in \Omega\ .\ 
			\mathbf{M}(\nu,\omega) &= \mathbf{M}(\omega,\nu)^*
		\end{align*}
		This proves that self-adjointness is equivalent to symmetry in the more restricive
		case of $\mathbf{M}\in\Omega^2\rightarrow\mathbb{R}$, too.
	\end{proof}

	\subsection{Eigenpairs and the Spectral Theorem}

	Now, recall the qualities of the so-called \emph{eigenpairs} of matrices: 
	\begin{definition}
		A vector $\mathbf{v} \in V$ from some vector space where $V\subseteq \Omega 
		\rightarrow \mathbf{C}$ is a right eigenvector of some matrix $\mathbf{M} \in 
		\Omega^2 \rightarrow \mathbb{C}$ with corresponding eigenvalue $\lambda$ if 
		$$
			\mathbf{M}\cdot\mathbf{v} = \lambda \mathbf{v}
		$$
		and an analogous definition applies to left eigenvectors.
	\end{definition}
	For example, the transition matrix $\mathbf{P}$ of any Markov chain has a left eigenvector 
	$\pi$ with eigenvalue $1$ if, and only if, $\pi$ is a stationary distribution of the Markov 
	chain. It is also important to note that, for any matrix $\mathbf{M}$, for any $n\in
	\mathbb{N}_0$, $\mathbf{M}^n$ has the same eigenvectors as $\mathbf{M}$, with corresponding 
	eigenvalues $\lambda^n$.

	The discussion of eigenvalues gives rise to the following important result, known as the 
	\emph{spectral} theorem; the use of the word spectral in the context of linear algebra 
	derives from the fact that the set of eigenvalues of a matrix is called its 
	\emph{spectrum}.
	\begin{theorem}[Spectral Theorem]
		\label{theorem:spectral}
		Given an inner product space $(V, + , \bullet, \langle \cdot,\cdot\rangle)$ with 
		respect to a field $F\subseteq \mathbb{C}$, where the ``dimension''\footnote{The 
		dimension of a vector space is the smallest cardinality of a subset $U \subseteq V
		$ such that $\forall \mathbf{v} \in V,  \mathbf{u} \in U. \exists \kappa_\mathbf{u}
		\in F . \sum_{\mathbf{u}\in U} \kappa_\mathbf{u}\bullet\mathbf{u}$. 
		We call such a subset $U$ a `basis' of the vector space.} 
		of $V$ is $n$. Then if a linear operator $M$ is self-adjoint 
		with respect to this inner product space, it has $n$ \emph{real} eigenvalues, 
		which we denote $\lambda_1 \geq \hdots \geq \lambda_n$, the corresponding right 
		eigenvectors $\{\mathbf{v}_i : i \in \{1\hdots n\}\}$ which form an orthonormal 
		basis for $V$.
	\end{theorem}
	Before we prove the spectral theorem, we must state and prove the following lemma as well:
	\begin{lemma}
		\label{lemma:spectralhelp}
		Let $(V, + , \bullet, \langle\cdot,\cdot\rangle)$ be an inner product space with 
		respect to field $F \subseteq \mathbb{C}$, and suppose that $V$ is of dimension $n 
		\in \mathbb{N}$. Then, for any vector $\mathbf{v}\in V$ such that $\mathbf{v}\neq 
		\mathbf{0}$, the set $U\subset V$ defined by $U \coloneqq \{\mathbf{u} : \mathbf{u}
		\bot \mathbf{v}\}$ is of dimension $n-1$.
	\end{lemma}
	\begin{proof}
		We prove that this is the case by bounding the dimension of $U$ both below and 
		above as $n-1$. So consider some basis $U'$ of $U$. Then take any $\mathbf{w} \in 
		V$, and note that with
		$$
			\mathbf{w}' \coloneqq \mathbf{w} - 
			\langle\mathbf{v},\mathbf{v}\rangle^{-1} \bullet 
			\langle\mathbf{w},\mathbf{v}\rangle\bullet \mathbf{v}
		$$
		We have that $\mathbf{w}' \bot \mathbf{v}$. Therefore, $\mathbf{w}' \in U$, and 
		can therefore be written as a linear combination of the vectors in $U'$. Since 
		this goes for any $\mathbf{w} \in V$, we may obtain a base for $V$ as $\{
		\mathbf{v}\} \cup U'$. Thus, since the smallest base for $V$ is of cardinality $n$ 
		by assumption, any base $U'$ of $U$ must have at least cardinality $n-1$, that is, 
		$\mathrm{dim} (U) \geq n-1$. \\
		Now take any minimal base $V'$ of $V$, that is, one containing $n$ vectors. Then,
		assume, without loss of generality, that $\mathbf{v} \in V'$, for if it were not, 
		we could write $\mathbf{v} = \sum_{\mathbf{v'} \in V'} a(\mathbf{v}') \cdot 
		\mathbf{v}'$, and take replace any $\mathbf{w} \in V'$ such that $a \coloneqq 
		a(\mathbf{w}) \neq 0$ by $\mathbf{v}$ by writing  $\mathbf{w} = a^{-1} \cdot 
		\left(\mathbf{v}- \sum_{\mathbf{v}' \in V'\setminus{\{\mathbf{w}\}}}a(\mathbf{v}') 
		\cdot\mathbf{v}'\right)$.\\ 
		Assume further, without loss of generality, that for each vector $\mathbf{w}\in V'$
		such that $\mathbf{w} \neq \mathbf{v}$, $\mathbf{w} \bot \mathbf{v}$. This is 
		justifiable since, if this is not already the case, we can simply replace each 
		such vector $\mathbf{w}$ with $\mathbf{w}'$ as above; this preserves the set being 
		a basis, but guarantees that all vectors in the basis are orthogonal to 
		$\mathbf{v}$. \\
		Note that the set being a minimum basis implies the vectors of $V'$ are linearly 
		independent. That is, none of them may be written as a linear combination of the 
		others. Then, take any $\mathbf{u}\in U$, and note that in expressing $\mathbf{u}$ 
		as a linear combination of $V'$, the co\"efficient $\kappa_\mathbf{v}$ of 
		$\mathbf{v}$ in the linear combination for $\mathbf{u}$ must be $0$, since we have 
		that
		\begin{align*}
			0 &= \langle \mathbf{u}, \mathbf{v} \rangle \\
			 &= \left(\sum_{\mathbf{w}\in V' \setminus\{\mathbf{v}\}}
			\langle \kappa_\mathbf{w}\bullet\mathbf{w},\mathbf{v}\rangle\right) + 
			 \langle \kappa_\mathbf{v} \bullet \mathbf{v} , \mathbf{v} \rangle \\
			&= 0 + \kappa_\mathbf{v} \cdot \langle \mathbf{v} , \mathbf{v} \rangle
		\end{align*}
		which is impossible for $\kappa_\mathbf{v} \neq 0$ since $\langle\mathbf{v},
		\mathbf{v}\rangle > 0$. \\
		Again, since this goes for all $\mathbf{u} \in U$, the vectors $V' \setminus 
		\{\mathbf{v}\}$ form a basis of cardinality $n-1$ for $U$, and so $\mathrm{dim}(U) 
		\geq n-1$, completing the proof.
	\end{proof}
	We may now proceed with the proof of the spectral theorem.
	\begin{proof}[Proof of theorem \ref{theorem:spectral}]
		We prove first that any eigenvalue of the linear operator is, indeed, real, then
		that the linear operator has $n$ corresponding, orthonromal eigenvectors.\\
		A result known as the \emph{fundamental theorem of linear algebra} states that 
		the linear operator has at least one eigenpair $(\lambda, \mathbf{v})$ where 
		$\mathbf{v} \neq \mathbf{0}$. Then note that since $\mathbf{v} \neq\mathbf{0}$,
		$\langle \mathbf{v},\mathbf{v}\rangle > 0$. Now, by self-adjointness, we have
		\begin{align*}
			\langle M(\mathbf{v}),\mathbf{v}\rangle &= 
			\langle \mathbf{v},M(\mathbf{v})\rangle \\
			\langle M(\mathbf{v}),\mathbf{v}\rangle &= 
			\langle M(\mathbf{v}),\mathbf{v}\rangle^* \\
			\langle \lambda \bullet \mathbf{v},\mathbf{v}\rangle &= 
			\langle \lambda \bullet \mathbf{v},\mathbf{v}\rangle^* \\
			\lambda \cdot \langle\mathbf{v},\mathbf{v}\rangle &=
			\lambda^* \cdot \langle\mathbf{v},\mathbf{v}\rangle^* \\
			\lambda \cdot \langle\mathbf{v},\mathbf{v}\rangle &=
			\lambda^* \cdot \langle\mathbf{v},\mathbf{v}\rangle 
		\end{align*}
		Therefore $\lambda$ must be real.\par
		Now, we proceed to show that the corresponding eigenvectors form an orthonormal%
		\footnote{the general definition of orthonormity replaces the $L^2_\pi$-norm 
		discussed previously with some other function $\|\cdot\|\in V\rightarrow F$ 
		satisfying $\|a\bullet \mathbf{w}\| = a\cdot \|\mathbf{w}\|$
		and requires  $\|\mathbf{v}\|= 1$, where $1$ is the multiplicative identity in $F$.} 
		basis for $V$, for which we proceed by induction on $n$. For the base case, where 
		$n = 1$, we have that we can simply take $\mathbf{v}_1 \coloneqq 
		\|\mathbf{v}\|^{-1}\bullet \mathbf{v}$ (where $a^{-1} \in F$ denotes the 
		multiplicative inverse of $a\in F$ and $\mathbf{v}$ is the eigenvector whose 
		existence is guaranteed by the fundamental theorem of linear algebra). \\
		Now assume this holds for $n-1$ and take for some vector space with dimension $n$ 
		the linear operator $M$ to be self-adjoint, and take $\mathbf{v}$ to be an 
		eigenvector for it. Then denote by $U$ the subset of $V$ that contains all vectors 
		$\mathbf{u}$ such that $\mathbf{u}\bot\mathbf{v}$. Denote by $M_U$ the linear 
		operator $M$ restricted to domain $U$, and note that, its range is also a subset 
		of $U$ since for any $\mathbf{u}\in U$,
		$$
			\langle \mathbf{v},M(\mathbf{u})\rangle =
			\langle M(\mathbf{v}),\mathbf{u}\rangle = 
			\lambda \cdot \langle\mathbf{v} , \mathbf{u}\rangle =
			0
		$$
		i.e.\ $\mathbf{v}\bot M_U(\mathbf{u})$, so $M_U(\mathbf{u})\in U$. Notice that $U$ 
		has dimension $n-1$ by lemma \ref{lemma:spectralhelp}. 
		Now, by the inductive hypothesis, 
		there is an orthonormal basis of $U$ consisting of $n-1$ right eigenvectors of 
		$M_U$, which are also right eigenvectors of $M$. Therefore, by extending this 
		basis by a normalised variant of $\mathbf{v}$ yields a basis for $V$, completing 
		the proof.
	\end{proof}

	Now, with respect to the specific inner product space $l_2(\Omega, \pi)$, where $\Omega$ 
	is finite, this implies that any self-adjoint linear operator in this inner product space 
	has $n \coloneqq |\Omega|$ \emph{eigenfunctions} (i.e.\ eigenvectors) $f_1\hdots f_n$ that 
	form an orthormal basis with respect to the norm $\|\cdot\|_{2,\pi}$ we can write any $g 
	\in \Omega\rightarrow \mathbb{R}$ as 
	$$
		g = \sum_{i=1}^{i=n} \langle g, f_i \rangle \bullet f_i
	$$
	by an argument similar to the one presented in the proof of lemma \ref{lemma:spectralhelp}.
	\\
	Furthermore, we can actually see that we can decompose each row the linear operator $M$ as
	$\mathbf{M}_x \coloneqq (\mathbf{M}(x,y))_{y\in\Omega}$ as
	$$
		\mathbf{M}_x = \lambda y\in\Omega\ .\ \sum_{i=1}^{i=n}\lambda_i f_i(x)f_i(y)\pi(y)
	$$
	since 
	\begin{align*}
		\frac{\mathbf{M}_x}{\pi}
		&\coloneqq\lambda y\in\Omega\ .\ \frac{\mathbf{M}_x(y)}{\pi(y)} \\
		&= \lambda y\in\Omega\ .\ \sum_{i=1}^{i=n} 
		\left\langle \frac{\mathbf{M}_x}{\pi} , f_i\right\rangle\cdot f_i(y) \\
		&= \lambda y\in\Omega\ .\ \sum_{i=1}^{i=n} 
		\left(\sum_{z\in\Omega}\frac{M(x,z) \cdot f_i(z) \cdot \pi(z)}{\pi(y)}\right)\cdot 
		f_i(y) \\
		&= \lambda y\in\Omega\ .\ M(f_i)(x) \cdot f_i(y) \\ 
		&= \lambda y\in\Omega\ .\ \lambda_i \cdot f_i(x) \cdot f_i(y)
	\end{align*}

	\subsection{Markov Chains and Linear Algebra}
	We now turn our attention to the interactions between linear algebra and Markov chains. The 
	first property of Markov chains that we will thus consider is that of \emph{reversibility}.

	\subsubsection{Reversible Markov Chains}
	\begin{definition}[Reversible Markov Chains]
		Suppose a Markov chaini $(X_t){t\in\mathbb{N}_0}$ over state space $\mathcal{I}$ 
		with transition matrix $\mathbf{P}$ has stationary dsitribution $\pi$ such that 
		$\forall u\in\mathcal{I} 
		. \pi(i) > 0$. Then, we call this Markov chain reversible if, for any $i,j \in
		\mathcal{I}$,
		$$
			\mathbb{P}(X_n = i \land X_{n+1} = j | X_n \sim \pi) 
			= 
			\mathbb{P}(X_n = j \land X_{n+1} = i | X_n \sim \pi),
		$$
		i.e.\ 
		$$
			\pi(i) P_{i, j} = \pi(j) P_{j,i}.
		$$
	\end{definition}
	
	Recall from Chapter 2 that any Markov chain could be vewed as some random walk over a 
	directed graph; with different `edge weights' not only on edges between distinct paris of 
	vertices, but also between the different directions of edges between one such pair. A 
	reversible Markov chain thus models some random walk over a weighted, but \emph{undirected} 
	graph, as the definition of reversibility implies that for any such random walk, any edge 
	in the graph is equally probable to be traversed in one direction as the other.\\
	The first property of reversible Markov chains with respect to linear algebra is recorded 
	in lemma \ref{lemma:reversibleselfadjoint}.
	\begin{lemma}
		\label{lemma:reversibleselfadjoint}
		A Markov chain over state space $\mathcal{I}$ with transition matrix $\mathbf{P}$ 
		and stationary distribution $\pi$ is reversible if, and only if,
		$\mathbf{P}$ is self-adjoint with respect to $l_2(\mathcal{I},\pi)$.
	\end{lemma}
	\begin{proof}
		Suppose first that the Markov chain is reversible. 
		Then, we have for any $f,g \in \mathcal{I} \in \mathbb{I}$
		\begin{align*}
			\langle \mathbf{P}\cdot f, g\rangle_\pi 
			&= \sum_{i,j\in \mathcal{I}} 
			P_{i,j}\cdot f(j) \cdot g(i) \cdot \pi(i)\\
			&= \sum_{i,j\in \mathcal{I}} 
			P_{j,i}\cdot f(j) \cdot g(i) \cdot \pi(j)\\
			&= \langle f, \mathbf{P}\cdot g\rangle_\pi.
		\end{align*}
		Secondly suppose that the matrix $\mathbf{P}$ is self-adjoint with respect to 
		$l_2(\mathcal{I}, \pi)$. Then we have that
		\begin{align*}
			\pi(j)P_{j,i} &= \langle \mathbf{P}\cdot 1[i], 1[j]\rangle_\pi\\
			&= \langle 1[i], \mathbf{P}\cdot1[j] \rangle_\pi \\
			&= \pi(i)P_{i,j}.
		\end{align*}
	\end{proof}

	Now, to re\"iterate: A reversible Markov chain models some random walk over an undirected 
	graph. From this model, we can derive the following facts, which are to be proven as part 
	of an example sheet. We therefore merely state them.
	\begin{claim}
		Suppose that $G = (V,E)$ is a finite undirected graph on $n$ vertices, and that 
		some random walk over $G$ is reversible. \\
		Then let $\lambda_1 \hdots \lambda_n$ be the (right) \emph{spectrum} of this random 
		walk's
		transition matrix and fix $\lambda_1 \geq \lambda_2 \geq \hdots \geq \lambda_n$ 
		(recall that all $\lambda_i$ are real by the spectral theorem).
		Then we have that 
		\begin{itemize}
			\item $\forall i . |\lambda_i| \leq 1$.
			\item $\lambda_1 = 1$.
			\item $\lambda_2 < 1$ if---and only if---the graph $G$ is connected.
			\item $\lambda_n = -1$ if---and only if---one of the graph's connected 
			components is bipartite.
		\end{itemize}
	\end{claim}
	With these facts, the following lemma is clearly also true:
	\begin{lemma}
		A random walk over an undirected graph $G$ with $n$ vertices converges to its 
		stationary distribution if, and only if, its spectrum $\lambda_1\geq\lambda_2\geq
		\hdots\geq\lambda_n$ satisifes
		$$
			\max_{2\leq i\leq n}(|\lambda_i|) < 1 .
		$$
	\end{lemma}
